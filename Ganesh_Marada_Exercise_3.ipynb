{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganesh1616/Ganesh_INFO5731_Fall2024/blob/main/Ganesh_Marada_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "78f9c098-4e37-4ec1-946d-e3003706f6b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOne of the more interesting text classification or text mining tasks is\\nanalyzing the customer evaluations for a product or service.Our goal in\\nthis study is to categorize customer reviews into good, negative,and\\nneutral attitudes based on the text content.The following list contains\\nfour different types of features that can be useful when building a machine learning model:\\n 1. Readability Score\\n 2. Named Entity Recognition\\n 3. Punctuation Counts\\n 4. Word Embeddings\\n\\nCombining these several variables helps a machine learning model to better understand the subtleties present in text data,\\nwhich makes it possible to forecast customer reviews sentiment more accurately. The way the features interact offers a\\nthorough comprehension of the text emotional content, and organization.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "One of the more interesting text classification or text mining tasks is\n",
        "analyzing the customer evaluations for a product or service.Our goal in\n",
        "this study is to categorize customer reviews into good, negative,and\n",
        "neutral attitudes based on the text content.The following list contains\n",
        "four different types of features that can be useful when building a machine learning model:\n",
        " 1. Readability Score\n",
        " 2. Named Entity Recognition\n",
        " 3. Punctuation Counts\n",
        " 4. Word Embeddings\n",
        "\n",
        "Combining these several variables helps a machine learning model to better understand the subtleties present in text data,\n",
        "which makes it possible to forecast customer reviews sentiment more accurately. The way the features interact offers a\n",
        "thorough comprehension of the text emotional content, and organization.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7829fd65-f8af-407e-9298-2c145abbe5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (71.0.4)\n",
            "Readability (Flesch-Kincaid): 7.2\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# 1.Readability Score\n",
        "!pip install textstat\n",
        "import textstat\n",
        "\n",
        "# Sample\n",
        "text = \"This product is fantastic! Definitely worth the price.\"\n",
        "\n",
        "def extract_readability(text):\n",
        "    return textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "# Calculate readability score\n",
        "readability = extract_readability(text)\n",
        "print(f\"Readability (Flesch-Kincaid): {readability}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Named Entity Recognition\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample\n",
        "text = \"The iPhone 14 by Apple was released in California last year.\"\n",
        "\n",
        "# Function to extract named entities using spaCy\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Extract named entities\n",
        "entities = extract_named_entities(text)\n",
        "print(f\"Named Entities: {entities}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3CZILxvvk1I",
        "outputId": "d0ef7f8f-ce44-4770-838c-b2930efd7724"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('14', 'CARDINAL'), ('Apple', 'ORG'), ('California', 'GPE'), ('last year', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Punctuation Counts\n",
        "import string\n",
        "\n",
        "# Sample\n",
        "text = \"This product is fantastic! Definitely worth the price.\"\n",
        "\n",
        "# Function to count punctuation marks\n",
        "def count_punctuation(text):\n",
        "    return {punc: text.count(punc) for punc in string.punctuation}\n",
        "\n",
        "# Count punctuation marks\n",
        "punctuation_counts = count_punctuation(text)\n",
        "print(f\"Punctuation Counts: {punctuation_counts}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEMyJRgWv6BP",
        "outputId": "c8c6f928-913e-4242-814d-03d6a8e29113"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation Counts: {'!': 1, '\"': 0, '#': 0, '$': 0, '%': 0, '&': 0, \"'\": 0, '(': 0, ')': 0, '*': 0, '+': 0, ',': 0, '-': 0, '.': 1, '/': 0, ':': 0, ';': 0, '<': 0, '=': 0, '>': 0, '?': 0, '@': 0, '[': 0, '\\\\': 0, ']': 0, '^': 0, '_': 0, '`': 0, '{': 0, '|': 0, '}': 0, '~': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Word Embeddings\n",
        "\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample\n",
        "texts = [\"This product is fantastic! It exceeded all my expectations.\",\n",
        "         \"Terrible experience. The item broke in a week.\",\n",
        "         \"I'm not sure if I like it or not. It's okay, I guess.\"]\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to train Word2Vec and generate embeddings\n",
        "def train_word2vec(corpus):\n",
        "    tokenized_text = [nltk.word_tokenize(text.lower()) for text in corpus]\n",
        "    model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = train_word2vec(texts)\n",
        "\n",
        "word_vector = word2vec_model.wv['fantastic'] if 'fantastic' in word2vec_model.wv else None\n",
        "print(f\"Vector for 'fantastic': {word_vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TTTG9r2wQ46",
        "outputId": "3bd30343-83cd-4599-be68-bf29952bdc61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'fantastic': [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
            "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
            " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
            "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
            " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
            "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
            "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
            "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
            "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
            " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
            " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
            " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
            " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
            " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
            "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
            " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
            " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
            " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
            "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
            " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
            " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
            "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
            " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
            "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
            "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6af9ca-4280-410f-e271-6be8cc21b44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features in descending order by Chi-Square score:\n",
            "Feature: product, Chi2 Score: 1.5000\n",
            "Feature: exceeded, Chi2 Score: 1.5000\n",
            "Feature: all, Chi2 Score: 1.5000\n",
            "Feature: is, Chi2 Score: 1.5000\n",
            "Feature: fantastic, Chi2 Score: 1.5000\n",
            "Feature: love, Chi2 Score: 1.5000\n",
            "Feature: expectations, Chi2 Score: 1.5000\n",
            "Feature: my, Chi2 Score: 1.5000\n",
            "Feature: quality, Chi2 Score: 1.5000\n",
            "Feature: amazing, Chi2 Score: 1.5000\n",
            "Feature: not, Chi2 Score: 1.3333\n",
            "Feature: sure, Chi2 Score: 0.6667\n",
            "Feature: terrible, Chi2 Score: 0.6667\n",
            "Feature: or, Chi2 Score: 0.6667\n",
            "Feature: okay, Chi2 Score: 0.6667\n",
            "Feature: never, Chi2 Score: 0.6667\n",
            "Feature: the, Chi2 Score: 0.6667\n",
            "Feature: again, Chi2 Score: 0.6667\n",
            "Feature: week, Chi2 Score: 0.6667\n",
            "Feature: like, Chi2 Score: 0.6667\n",
            "Feature: item, Chi2 Score: 0.6667\n",
            "Feature: in, Chi2 Score: 0.6667\n",
            "Feature: if, Chi2 Score: 0.6667\n",
            "Feature: guess, Chi2 Score: 0.6667\n",
            "Feature: from, Chi2 Score: 0.6667\n",
            "Feature: experience, Chi2 Score: 0.6667\n",
            "Feature: buying, Chi2 Score: 0.6667\n",
            "Feature: broke, Chi2 Score: 0.6667\n",
            "Feature: brand, Chi2 Score: 0.6667\n",
            "Feature: awful, Chi2 Score: 0.6667\n",
            "Feature: it, Chi2 Score: 0.1667\n",
            "Feature: this, Chi2 Score: 0.0833\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data and labels (1 for positive, 0 for negative)\n",
        "texts = [\"This product is fantastic! It exceeded all my expectations.\",\n",
        "         \"Terrible experience. The item broke in a week.\",\n",
        "         \"I'm not sure if I like it or not. It's okay, I guess.\",\n",
        "         \"Amazing quality! I love it.\",\n",
        "         \"Awful! Never buying from this brand again.\"]\n",
        "\n",
        "# Labels (1 for positive sentiment, 0 for negative sentiment)\n",
        "labels = [1, 0, 0, 1, 0]\n",
        "\n",
        "# Converting text into features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)  # Text data into a document-term matrix\n",
        "\n",
        "# Perform Chi-Square Test for feature selection\n",
        "chi2_scores, p_values = chi2(X, labels)\n",
        "\n",
        "# Converting Chi-Square scores into a DataFrame for better visualization\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi2 Score': chi2_scores})\n",
        "\n",
        "# Sorting features by their Chi-Square score in descending order\n",
        "sorted_chi2_df = chi2_df.sort_values(by='Chi2 Score', ascending=False)\n",
        "\n",
        "# Display the features by Chi-Square score in descending order\n",
        "print(\"Features in descending order by Chi-Square score:\")\n",
        "for idx, row in sorted_chi2_df.iterrows():\n",
        "    print(f\"Feature: {row['Feature']}, Chi2 Score: {row['Chi2 Score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb21d79-ad32-4bf5-cfdd-574b2346f3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (71.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Similarity to Query:\n",
            "\n",
            "Rank 1: Similarity = 0.6995\n",
            "Text: This movie is absolutely fantastic! I loved every moment of it.\n",
            "Readability (Flesch-Kincaid): 5.4\n",
            "Named Entities: []\n",
            "Punctuation Counts: {'!': 1, '\"': 0, '#': 0, '$': 0, '%': 0, '&': 0, \"'\": 0, '(': 0, ')': 0, '*': 0, '+': 0, ',': 0, '-': 0, '.': 1, '/': 0, ':': 0, ';': 0, '<': 0, '=': 0, '>': 0, '?': 0, '@': 0, '[': 0, '\\\\': 0, ']': 0, '^': 0, '_': 0, '`': 0, '{': 0, '|': 0, '}': 0, '~': 0}\n",
            "Word Vector for 'fantastic': [-8.7316148e-03  2.1298144e-03 -8.7987975e-04 -9.3186898e-03\n",
            " -9.4286399e-03 -1.4179149e-03  4.4312663e-03  3.7077719e-03\n",
            " -6.4946748e-03 -6.8761185e-03 -4.9945172e-03 -2.2946892e-03\n",
            " -7.2474359e-03 -9.5957266e-03 -2.7421629e-03 -8.3696162e-03\n",
            " -6.0349167e-03 -5.6702397e-03 -2.3409310e-03 -1.7052221e-03\n",
            " -8.9534400e-03 -7.3043123e-04  8.1534041e-03  7.6926486e-03\n",
            " -7.2023715e-03 -3.6695846e-03  3.1139941e-03 -9.5680868e-03\n",
            "  1.4714609e-03  6.5195062e-03  5.7492927e-03 -8.7652020e-03\n",
            " -4.5096735e-03 -8.1445808e-03  3.9516675e-05  9.2736166e-03\n",
            "  5.9745889e-03  5.0695091e-03  5.0567775e-03 -3.2421269e-03\n",
            "  9.5505519e-03 -7.3566004e-03 -7.2673736e-03 -2.2658040e-03\n",
            " -7.7482557e-04 -3.2194895e-03 -5.9552921e-04  7.4853566e-03\n",
            " -6.9235708e-04 -1.6228202e-03  2.7476936e-03 -8.3567500e-03\n",
            "  7.8578619e-03  8.5355248e-03 -9.5826900e-03  2.4440179e-03\n",
            "  9.9085001e-03 -7.6623862e-03 -6.9638449e-03 -7.7404589e-03\n",
            "  8.3969980e-03 -6.8056927e-04  9.1506224e-03 -8.1567671e-03\n",
            "  3.7380690e-03  2.6359092e-03  7.3567848e-04  2.3282929e-03\n",
            " -7.4728210e-03 -9.3528153e-03  2.3596033e-03  6.1523607e-03\n",
            "  7.9881232e-03  5.7357023e-03 -7.6453364e-04  8.3104465e-03\n",
            " -9.3392525e-03  3.4115666e-03  2.6341304e-04  3.8610569e-03\n",
            "  7.3794192e-03 -6.7266328e-03  5.5815112e-03 -9.5176324e-03\n",
            " -8.0188824e-04 -8.6877262e-03 -5.0960151e-03  9.2895012e-03\n",
            " -1.8555666e-03  2.9209265e-03  9.0698572e-03  8.9408262e-03\n",
            " -8.2058432e-03 -3.0167706e-03  9.8948088e-03  5.1054070e-03\n",
            " -1.5882028e-03 -8.6902166e-03  2.9573061e-03 -6.6785072e-03]\n",
            "\n",
            "Rank 2: Similarity = 0.6606\n",
            "Text: I'm not sure why people like this movie; it's quite boring.\n",
            "Readability (Flesch-Kincaid): 2.9\n",
            "Named Entities: []\n",
            "Punctuation Counts: {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': 0, \"'\": 2, '(': 0, ')': 0, '*': 0, '+': 0, ',': 0, '-': 0, '.': 1, '/': 0, ':': 0, ';': 1, '<': 0, '=': 0, '>': 0, '?': 0, '@': 0, '[': 0, '\\\\': 0, ']': 0, '^': 0, '_': 0, '`': 0, '{': 0, '|': 0, '}': 0, '~': 0}\n",
            "Word Vector for 'fantastic': [-8.7316148e-03  2.1298144e-03 -8.7987975e-04 -9.3186898e-03\n",
            " -9.4286399e-03 -1.4179149e-03  4.4312663e-03  3.7077719e-03\n",
            " -6.4946748e-03 -6.8761185e-03 -4.9945172e-03 -2.2946892e-03\n",
            " -7.2474359e-03 -9.5957266e-03 -2.7421629e-03 -8.3696162e-03\n",
            " -6.0349167e-03 -5.6702397e-03 -2.3409310e-03 -1.7052221e-03\n",
            " -8.9534400e-03 -7.3043123e-04  8.1534041e-03  7.6926486e-03\n",
            " -7.2023715e-03 -3.6695846e-03  3.1139941e-03 -9.5680868e-03\n",
            "  1.4714609e-03  6.5195062e-03  5.7492927e-03 -8.7652020e-03\n",
            " -4.5096735e-03 -8.1445808e-03  3.9516675e-05  9.2736166e-03\n",
            "  5.9745889e-03  5.0695091e-03  5.0567775e-03 -3.2421269e-03\n",
            "  9.5505519e-03 -7.3566004e-03 -7.2673736e-03 -2.2658040e-03\n",
            " -7.7482557e-04 -3.2194895e-03 -5.9552921e-04  7.4853566e-03\n",
            " -6.9235708e-04 -1.6228202e-03  2.7476936e-03 -8.3567500e-03\n",
            "  7.8578619e-03  8.5355248e-03 -9.5826900e-03  2.4440179e-03\n",
            "  9.9085001e-03 -7.6623862e-03 -6.9638449e-03 -7.7404589e-03\n",
            "  8.3969980e-03 -6.8056927e-04  9.1506224e-03 -8.1567671e-03\n",
            "  3.7380690e-03  2.6359092e-03  7.3567848e-04  2.3282929e-03\n",
            " -7.4728210e-03 -9.3528153e-03  2.3596033e-03  6.1523607e-03\n",
            "  7.9881232e-03  5.7357023e-03 -7.6453364e-04  8.3104465e-03\n",
            " -9.3392525e-03  3.4115666e-03  2.6341304e-04  3.8610569e-03\n",
            "  7.3794192e-03 -6.7266328e-03  5.5815112e-03 -9.5176324e-03\n",
            " -8.0188824e-04 -8.6877262e-03 -5.0960151e-03  9.2895012e-03\n",
            " -1.8555666e-03  2.9209265e-03  9.0698572e-03  8.9408262e-03\n",
            " -8.2058432e-03 -3.0167706e-03  9.8948088e-03  5.1054070e-03\n",
            " -1.5882028e-03 -8.6902166e-03  2.9573061e-03 -6.6785072e-03]\n",
            "\n",
            "Rank 3: Similarity = 0.6334\n",
            "Text: This product is fantastic! Definitely worth the price.\n",
            "Readability (Flesch-Kincaid): 7.2\n",
            "Named Entities: []\n",
            "Punctuation Counts: {'!': 1, '\"': 0, '#': 0, '$': 0, '%': 0, '&': 0, \"'\": 0, '(': 0, ')': 0, '*': 0, '+': 0, ',': 0, '-': 0, '.': 1, '/': 0, ':': 0, ';': 0, '<': 0, '=': 0, '>': 0, '?': 0, '@': 0, '[': 0, '\\\\': 0, ']': 0, '^': 0, '_': 0, '`': 0, '{': 0, '|': 0, '}': 0, '~': 0}\n",
            "Word Vector for 'fantastic': [-8.7316148e-03  2.1298144e-03 -8.7987975e-04 -9.3186898e-03\n",
            " -9.4286399e-03 -1.4179149e-03  4.4312663e-03  3.7077719e-03\n",
            " -6.4946748e-03 -6.8761185e-03 -4.9945172e-03 -2.2946892e-03\n",
            " -7.2474359e-03 -9.5957266e-03 -2.7421629e-03 -8.3696162e-03\n",
            " -6.0349167e-03 -5.6702397e-03 -2.3409310e-03 -1.7052221e-03\n",
            " -8.9534400e-03 -7.3043123e-04  8.1534041e-03  7.6926486e-03\n",
            " -7.2023715e-03 -3.6695846e-03  3.1139941e-03 -9.5680868e-03\n",
            "  1.4714609e-03  6.5195062e-03  5.7492927e-03 -8.7652020e-03\n",
            " -4.5096735e-03 -8.1445808e-03  3.9516675e-05  9.2736166e-03\n",
            "  5.9745889e-03  5.0695091e-03  5.0567775e-03 -3.2421269e-03\n",
            "  9.5505519e-03 -7.3566004e-03 -7.2673736e-03 -2.2658040e-03\n",
            " -7.7482557e-04 -3.2194895e-03 -5.9552921e-04  7.4853566e-03\n",
            " -6.9235708e-04 -1.6228202e-03  2.7476936e-03 -8.3567500e-03\n",
            "  7.8578619e-03  8.5355248e-03 -9.5826900e-03  2.4440179e-03\n",
            "  9.9085001e-03 -7.6623862e-03 -6.9638449e-03 -7.7404589e-03\n",
            "  8.3969980e-03 -6.8056927e-04  9.1506224e-03 -8.1567671e-03\n",
            "  3.7380690e-03  2.6359092e-03  7.3567848e-04  2.3282929e-03\n",
            " -7.4728210e-03 -9.3528153e-03  2.3596033e-03  6.1523607e-03\n",
            "  7.9881232e-03  5.7357023e-03 -7.6453364e-04  8.3104465e-03\n",
            " -9.3392525e-03  3.4115666e-03  2.6341304e-04  3.8610569e-03\n",
            "  7.3794192e-03 -6.7266328e-03  5.5815112e-03 -9.5176324e-03\n",
            " -8.0188824e-04 -8.6877262e-03 -5.0960151e-03  9.2895012e-03\n",
            " -1.8555666e-03  2.9209265e-03  9.0698572e-03  8.9408262e-03\n",
            " -8.2058432e-03 -3.0167706e-03  9.8948088e-03  5.1054070e-03\n",
            " -1.5882028e-03 -8.6902166e-03  2.9573061e-03 -6.6785072e-03]\n",
            "\n",
            "Rank 4: Similarity = 0.5911\n",
            "Text: The restaurant had great service, but the food was disappointing.\n",
            "Readability (Flesch-Kincaid): 6.0\n",
            "Named Entities: []\n",
            "Punctuation Counts: {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': 0, \"'\": 0, '(': 0, ')': 0, '*': 0, '+': 0, ',': 1, '-': 0, '.': 1, '/': 0, ':': 0, ';': 0, '<': 0, '=': 0, '>': 0, '?': 0, '@': 0, '[': 0, '\\\\': 0, ']': 0, '^': 0, '_': 0, '`': 0, '{': 0, '|': 0, '}': 0, '~': 0}\n",
            "Word Vector for 'fantastic': [-8.7316148e-03  2.1298144e-03 -8.7987975e-04 -9.3186898e-03\n",
            " -9.4286399e-03 -1.4179149e-03  4.4312663e-03  3.7077719e-03\n",
            " -6.4946748e-03 -6.8761185e-03 -4.9945172e-03 -2.2946892e-03\n",
            " -7.2474359e-03 -9.5957266e-03 -2.7421629e-03 -8.3696162e-03\n",
            " -6.0349167e-03 -5.6702397e-03 -2.3409310e-03 -1.7052221e-03\n",
            " -8.9534400e-03 -7.3043123e-04  8.1534041e-03  7.6926486e-03\n",
            " -7.2023715e-03 -3.6695846e-03  3.1139941e-03 -9.5680868e-03\n",
            "  1.4714609e-03  6.5195062e-03  5.7492927e-03 -8.7652020e-03\n",
            " -4.5096735e-03 -8.1445808e-03  3.9516675e-05  9.2736166e-03\n",
            "  5.9745889e-03  5.0695091e-03  5.0567775e-03 -3.2421269e-03\n",
            "  9.5505519e-03 -7.3566004e-03 -7.2673736e-03 -2.2658040e-03\n",
            " -7.7482557e-04 -3.2194895e-03 -5.9552921e-04  7.4853566e-03\n",
            " -6.9235708e-04 -1.6228202e-03  2.7476936e-03 -8.3567500e-03\n",
            "  7.8578619e-03  8.5355248e-03 -9.5826900e-03  2.4440179e-03\n",
            "  9.9085001e-03 -7.6623862e-03 -6.9638449e-03 -7.7404589e-03\n",
            "  8.3969980e-03 -6.8056927e-04  9.1506224e-03 -8.1567671e-03\n",
            "  3.7380690e-03  2.6359092e-03  7.3567848e-04  2.3282929e-03\n",
            " -7.4728210e-03 -9.3528153e-03  2.3596033e-03  6.1523607e-03\n",
            "  7.9881232e-03  5.7357023e-03 -7.6453364e-04  8.3104465e-03\n",
            " -9.3392525e-03  3.4115666e-03  2.6341304e-04  3.8610569e-03\n",
            "  7.3794192e-03 -6.7266328e-03  5.5815112e-03 -9.5176324e-03\n",
            " -8.0188824e-04 -8.6877262e-03 -5.0960151e-03  9.2895012e-03\n",
            " -1.8555666e-03  2.9209265e-03  9.0698572e-03  8.9408262e-03\n",
            " -8.2058432e-03 -3.0167706e-03  9.8948088e-03  5.1054070e-03\n",
            " -1.5882028e-03 -8.6902166e-03  2.9573061e-03 -6.6785072e-03]\n",
            "\n",
            "Rank 5: Similarity = 0.4779\n",
            "Text: The iPhone 14 by Apple was released in California last year.\n",
            "Readability (Flesch-Kincaid): 6.4\n",
            "Named Entities: [('14', 'CARDINAL'), ('Apple', 'ORG'), ('California', 'GPE'), ('last year', 'DATE')]\n",
            "Punctuation Counts: {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': 0, \"'\": 0, '(': 0, ')': 0, '*': 0, '+': 0, ',': 0, '-': 0, '.': 1, '/': 0, ':': 0, ';': 0, '<': 0, '=': 0, '>': 0, '?': 0, '@': 0, '[': 0, '\\\\': 0, ']': 0, '^': 0, '_': 0, '`': 0, '{': 0, '|': 0, '}': 0, '~': 0}\n",
            "Word Vector for 'fantastic': [-8.7316148e-03  2.1298144e-03 -8.7987975e-04 -9.3186898e-03\n",
            " -9.4286399e-03 -1.4179149e-03  4.4312663e-03  3.7077719e-03\n",
            " -6.4946748e-03 -6.8761185e-03 -4.9945172e-03 -2.2946892e-03\n",
            " -7.2474359e-03 -9.5957266e-03 -2.7421629e-03 -8.3696162e-03\n",
            " -6.0349167e-03 -5.6702397e-03 -2.3409310e-03 -1.7052221e-03\n",
            " -8.9534400e-03 -7.3043123e-04  8.1534041e-03  7.6926486e-03\n",
            " -7.2023715e-03 -3.6695846e-03  3.1139941e-03 -9.5680868e-03\n",
            "  1.4714609e-03  6.5195062e-03  5.7492927e-03 -8.7652020e-03\n",
            " -4.5096735e-03 -8.1445808e-03  3.9516675e-05  9.2736166e-03\n",
            "  5.9745889e-03  5.0695091e-03  5.0567775e-03 -3.2421269e-03\n",
            "  9.5505519e-03 -7.3566004e-03 -7.2673736e-03 -2.2658040e-03\n",
            " -7.7482557e-04 -3.2194895e-03 -5.9552921e-04  7.4853566e-03\n",
            " -6.9235708e-04 -1.6228202e-03  2.7476936e-03 -8.3567500e-03\n",
            "  7.8578619e-03  8.5355248e-03 -9.5826900e-03  2.4440179e-03\n",
            "  9.9085001e-03 -7.6623862e-03 -6.9638449e-03 -7.7404589e-03\n",
            "  8.3969980e-03 -6.8056927e-04  9.1506224e-03 -8.1567671e-03\n",
            "  3.7380690e-03  2.6359092e-03  7.3567848e-04  2.3282929e-03\n",
            " -7.4728210e-03 -9.3528153e-03  2.3596033e-03  6.1523607e-03\n",
            "  7.9881232e-03  5.7357023e-03 -7.6453364e-04  8.3104465e-03\n",
            " -9.3392525e-03  3.4115666e-03  2.6341304e-04  3.8610569e-03\n",
            "  7.3794192e-03 -6.7266328e-03  5.5815112e-03 -9.5176324e-03\n",
            " -8.0188824e-04 -8.6877262e-03 -5.0960151e-03  9.2895012e-03\n",
            " -1.8555666e-03  2.9209265e-03  9.0698572e-03  8.9408262e-03\n",
            " -8.2058432e-03 -3.0167706e-03  9.8948088e-03  5.1054070e-03\n",
            " -1.5882028e-03 -8.6902166e-03  2.9573061e-03 -6.6785072e-03]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# Install required libraries\n",
        "!pip install transformers textstat spacy nltk gensim\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textstat\n",
        "import spacy\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download NLTK tokenizer data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "sample_data = [\n",
        "    \"This product is fantastic! Definitely worth the price.\",\n",
        "    \"The iPhone 14 by Apple was released in California last year.\",\n",
        "    \"This movie is absolutely fantastic! I loved every moment of it.\",\n",
        "    \"I'm not sure why people like this movie; it's quite boring.\",\n",
        "    \"The restaurant had great service, but the food was disappointing.\",\n",
        "]\n",
        "\n",
        "# Query (replace this with your actual query)\n",
        "query = \"I want to watch a great movie tonight.\"\n",
        "\n",
        "# 1. Readability Score\n",
        "def extract_readability(text):\n",
        "    return textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "# 2. Named Entity Recognition\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# 3. Punctuation Counts\n",
        "def count_punctuation(text):\n",
        "    return {punc: text.count(punc) for punc in string.punctuation}\n",
        "\n",
        "# 4. Word Embeddings (Word2Vec)\n",
        "def train_word2vec(corpus):\n",
        "    tokenized_text = [nltk.word_tokenize(text.lower()) for text in corpus]\n",
        "    model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model\n",
        "\n",
        "# Train Word2Vec model on the sample data\n",
        "word2vec_model = train_word2vec(sample_data)\n",
        "\n",
        "# 5. BERT Embeddings for Similarity\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def get_bert_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Encode query and sample data into BERT embeddings\n",
        "query_embedding = get_bert_embeddings(query)\n",
        "sample_embeddings = [get_bert_embeddings(text) for text in sample_data]\n",
        "\n",
        "# Calculate cosine similarity between query and each sample\n",
        "similarities = cosine_similarity([query_embedding], sample_embeddings)[0]\n",
        "\n",
        "# Rank the similarities in descending order\n",
        "ranking = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print ranked results along with readability, named entities, punctuation counts, and word embeddings\n",
        "print(\"Ranked Documents based on Similarity to Query:\\n\")\n",
        "for rank, (index, similarity) in enumerate(ranking):\n",
        "    text = sample_data[index]\n",
        "    readability = extract_readability(text)\n",
        "    entities = extract_named_entities(text)\n",
        "    punctuation_counts = count_punctuation(text)\n",
        "    word_vector = word2vec_model.wv['fantastic'] if 'fantastic' in word2vec_model.wv else None\n",
        "\n",
        "    print(f\"Rank {rank + 1}: Similarity = {similarity:.4f}\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Readability (Flesch-Kincaid): {readability}\")\n",
        "    print(f\"Named Entities: {entities}\")\n",
        "    print(f\"Punctuation Counts: {punctuation_counts}\")\n",
        "    print(f\"Word Vector for 'fantastic': {word_vector}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Gaining knowledge about different NLP strategies was made possible by working on feature extraction from text data.\n",
        "Because BERT can capture contextual meaning and improve document rating accuracy, it is very useful for text similarity.\n",
        "Word embeddings' representation of individual words was highlighted by Word2Vec.\n",
        "Keeping up with BERT's computational demands and guaranteeing uniform tokenization across models were two major obstacles.\n",
        "The challenges were managing the computational demands of BERT, integrating many tools, and guaranteeing uniform tokenization across libraries.\n",
        "Notwithstanding these challenges, the exercise was helpful in applying NLP principles to practical problems like text classification and information retrieval.\n",
        "My understanding of how to integrate various variables for more complex document ranking and analysis in data science has improved as a result of the exercise.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "cb573c0d-4518-4090-d6c2-fcf150cb4650"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nGaining knowledge about different NLP strategies was made possible by working on feature extraction from text data. \\nBecause BERT can capture contextual meaning and improve document rating accuracy, it is very useful for text similarity. \\nWord embeddings' representation of individual words was highlighted by Word2Vec. \\nKeeping up with BERT's computational demands and guaranteeing uniform tokenization across models were two major obstacles.\\nThe challenges were managing the computational demands of BERT, integrating many tools, and guaranteeing uniform tokenization across libraries.\\nNotwithstanding these challenges, the exercise was helpful in applying NLP principles to practical problems like text classification and information retrieval. \\nMy understanding of how to integrate various variables for more complex document ranking and analysis in data science has improved as a result of the exercise.\\n\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}